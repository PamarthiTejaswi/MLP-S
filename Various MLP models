import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
# 1. Create Dataset (y = x^2 + noise)
x = torch.tensor([[-4], [-3], [-2], [-1], [0], [1], [2], [3], [4], [5]], dtype=torch.float32)
y = torch.tensor([[15], [8.5], [4.2], [0.8], [0.1], [1.3], [3.7], [9.2], [16.5], [24.8]],
dtype=torch.float32)
# Train/Test split
x_train, y_train = x[:7], y[:7] # first 7 points
x_test, y_test = x[7:], y[7:] # last 3 points
# 2. Define MLP Architectures
class SmallMLP(nn.Module): # underfits
def __init__(self):
super().__init__()
self.net = nn.Sequential(
nn.Linear(1, 2),
nn.ReLU(),
nn.Linear(2, 1)
)
def forward(self, x): return self.net(x)
class MediumMLP(nn.Module): # good fit
def __init__(self):
super().__init__()
self.net = nn.Sequential(
nn.Linear(1, 16),
nn.ReLU(),
nn.Linear(16, 8),
nn.ReLU(),
nn.Linear(8, 1)
)
def forward(self, x): return self.net(x)
class HugeMLP(nn.Module): # overfits
def __init__(self):
super().__init__()
self.net = nn.Sequential(
nn.Linear(1, 128),
nn.ReLU(),
nn.Linear(128, 128),
nn.ReLU(),
nn.Linear(128, 128),
nn.ReLU(),
nn.Linear(128, 128),
nn.ReLU(),
nn.Linear(128, 1)
)
def forward(self, x): return self.net(x)
# 3. Training Function
def train_model(model, x_train, y_train, x_test, y_test, epochs=200, lr=0.01):
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)
train_losses, test_losses = [], []
for epoch in range(epochs):
# Training step
model.train()
optimizer.zero_grad()
y_pred = model(x_train)
loss = criterion(y_pred, y_train)
loss.backward()
optimizer.step()
# Evaluation step
model.eval()
with torch.no_grad():
train_losses.append(loss.item())
test_loss = criterion(model(x_test), y_test).item()
test_losses.append(test_loss)
return train_losses, test_losses
# 4. Train all three models
small_model = SmallMLP()
medium_model = MediumMLP()
huge_model = HugeMLP()
losses_small = train_model(small_model, x_train, y_train, x_test, y_test)
losses_medium = train_model(medium_model, x_train, y_train, x_test, y_test)
losses_huge = train_model(huge_model, x_train, y_train, x_test, y_test)
# 5. Plot Training vs Test Loss
plt.figure(figsize=(12,6))
# Small MLP
plt.subplot(1,3,1)
plt.plot(losses_small[0], label="Train Loss")
plt.plot(losses_small[1], label="Test Loss")
plt.title("Small MLP (Underfits)")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
# Medium MLP
plt.subplot(1,3,2)
plt.plot(losses_medium[0], label="Train Loss")
plt.plot(losses_medium[1], label="Test Loss")
plt.title("Medium MLP (Good Fit)")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
# Huge MLP
plt.subplot(1,3,3)
plt.plot(losses_huge[0], label="Train Loss")
plt.plot(losses_huge[1], label="Test Loss")
plt.title("Huge MLP (Overfits)")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.tight_layout()
plt.show() 



